{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "17e92a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,\n",
       " Dataset FashionMNIST\n",
       "     Number of datapoints: 10000\n",
       "     Root location: fashion-mnist\n",
       "     Split: Test\n",
       "     StandardTransform\n",
       " Transform: ToTensor(),\n",
       " torchvision.datasets.mnist.FashionMNIST)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#读取数据\n",
    "import torch\n",
    "import os\n",
    "import torchvision\n",
    "def load_data():\n",
    "    dataset=torchvision.datasets.FashionMNIST(root=\"fashion-mnist\",train=False,transform=torchvision.transforms.ToTensor(),download=True)\n",
    "    return dataset\n",
    "#初始化数据集\n",
    "dataset=load_data()\n",
    "len(dataset),dataset,type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b5fd0e99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,\n",
       " tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2353, 0.9333,\n",
       "         0.8824, 0.8588, 0.9294, 0.9843, 0.5961, 0.3765, 0.9333, 0.9686, 0.8863,\n",
       "         0.8549, 0.8980, 0.8431, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000]),\n",
       " tensor([6]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#数据集加载器\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=1,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "x, y = next(iter(loader))\n",
    "len(loader), x[0,0,3], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "42a20247",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (u): Linear(in_features=28, out_features=256, bias=True)\n",
       "  (w): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (v): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (tanh): Tanh()\n",
       "  (LogSoftmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nn.Linear实现RNN神经网络\n",
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        self.u = torch.nn.Linear(28, 256)\n",
    "        self.w = torch.nn.Linear(256, 256)\n",
    "        self.v = torch.nn.Linear(256, 10)\n",
    "        self.tanh = torch.nn.Tanh()#激活函数\n",
    "        self.LogSoftmax = torch.nn.LogSoftmax(dim=1)#定义归一化函数，对应了torch.nn.NLLLoss()\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        \n",
    "        u_x = self.u(inputs)#输入层\n",
    "        hidden = self.w(hidden)\n",
    "        hidden = self.tanh(hidden + u_x)#隐藏层和输入相加传入激活函数，并更新隐藏层\n",
    "        output = self.LogSoftmax(self.v(hidden))#归一化输出\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):#初始化隐藏层\n",
    "        return torch.zeros(1, 256)\n",
    "\n",
    "rnn=RNN()\n",
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b889a31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP=FP=TN=FN=overall=0\n",
    "\n",
    "def count(x,y,target_label):\n",
    "    global TP,FP,TN,FN,overall\n",
    "    if x==y:\n",
    "        overall+=1\n",
    "    if x==target_label and y==target_label:\n",
    "        TP+=1\n",
    "    elif x==target_label and y!=target_label:\n",
    "        FP+=1    \n",
    "    elif x!=target_label and y!=target_label:\n",
    "        TN+=1        \n",
    "    else:\n",
    "        FN+=1        \n",
    "\n",
    "def printParameter():\n",
    "    global TP,FP,TN,FN,overall\n",
    "    overallAccuracy=overall/(TP+TN+FP+FN)if (TP+TN+FP+FN) > 0 else 0\n",
    "    accuracy=(TP+TN)/(TP+TN+FP+FN)if (TP+TN+FP+FN) > 0 else 0\n",
    "    precision=TP/(TP+FP)if (TP + FP) > 0 else 0\n",
    "    recall=TP/(TP+FN)if (TP+FN) > 0 else 0\n",
    "    F1=2*(precision*recall)/(precision+recall)if (precision+recall) > 0 else 0\n",
    "    print(f\"overall accuracy={overallAccuracy}\")\n",
    "    print(f\"accuracy={accuracy}\")\n",
    "    print(f\"precision={precision}\")\n",
    "    print(f\"recall={recall}\")\n",
    "    print(f\"F1={F1}\")\n",
    "\n",
    "def resetCount():\n",
    "    global TP,FP,TN,FN,overall\n",
    "    TP=TN=FP=FN=overall=0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "233d2672",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#训练\n",
    "TP=TN=FP=FN=overall=0\n",
    "target_label=0#自定义正例，其他为负例\n",
    "rnn.cuda()\n",
    "import tqdm\n",
    "def train():\n",
    "    optimizer = torch.optim.Adam(rnn.parameters(), lr=1e-4)\n",
    "    criterion = torch.nn.NLLLoss()\n",
    "    rnn.train()\n",
    "    \n",
    "    bar=tqdm.tqdm(range(5))\n",
    "    hidden=rnn.initHidden().cuda()\n",
    "    hidden = hidden.cuda()\n",
    "    \n",
    "    for epoch in bar:\n",
    "        #rnn.zero_grad()\n",
    "        for i ,( x , y )in enumerate(loader):\n",
    "            \n",
    "            x=x.cuda()\n",
    "            y=y.cuda()\n",
    "            for j in range(x.shape[2]):\n",
    "                output, hidden = rnn(x[0,0,j], hidden)\n",
    "            count(output.argmax(dim=1),y.item(),target_label)\n",
    "            \n",
    "            if i%100==0:\n",
    "                print(i)\n",
    "                printParameter()\n",
    "                resetCount()\n",
    "            \n",
    "            loss = criterion(output,y)\n",
    "            loss.backward(retain_graph=True)\n",
    "            \"\"\"在PyTorch中，当你执行.backward()方法时，计算图上的所有中间张量都会被释放，\n",
    "            以节省内存。如果你需要多次进行反向传播，或者在反向传播后需要访问这些中间张量，\n",
    "            你需要设置retain_graph=True参数。\"\"\"\n",
    "            for p in rnn.parameters():\n",
    "                p.data-=learning_rate*p.grad.data\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    return output,loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "77901c01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "overall accuracy=0.0\n",
      "accuracy=0.0\n",
      "precision=0.0\n",
      "recall=0\n",
      "F1=0\n",
      "100\n",
      "overall accuracy=0.34\n",
      "accuracy=0.89\n",
      "precision=0.45454545454545453\n",
      "recall=0.5\n",
      "F1=0.47619047619047616\n",
      "200\n",
      "overall accuracy=0.3\n",
      "accuracy=0.8\n",
      "precision=0.3157894736842105\n",
      "recall=0.46153846153846156\n",
      "F1=0.37499999999999994\n",
      "300\n",
      "overall accuracy=0.38\n",
      "accuracy=0.88\n",
      "precision=0.3333333333333333\n",
      "recall=0.5\n",
      "F1=0.4\n",
      "400\n",
      "overall accuracy=0.39\n",
      "accuracy=0.91\n",
      "precision=0.45454545454545453\n",
      "recall=0.625\n",
      "F1=0.5263157894736842\n",
      "500\n",
      "overall accuracy=0.39\n",
      "accuracy=0.91\n",
      "precision=0.75\n",
      "recall=0.2727272727272727\n",
      "F1=0.39999999999999997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/5 [12:50<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m rnn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[45], line 31\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m     resetCount()\n\u001b[0;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output,y)\n\u001b[1;32m---> 31\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"在PyTorch中，当你执行.backward()方法时，计算图上的所有中间张量都会被释放，\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03m以节省内存。如果你需要多次进行反向传播，或者在反向传播后需要访问这些中间张量，\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03m你需要设置retain_graph=True参数。\"\"\"\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m rnn\u001b[38;5;241m.\u001b[39mparameters():\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch2\\lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch2\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnn = torch.load('model2')\n",
    "learning_rate = 0.001\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b51f2925",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(rnn, 'model2')#硬件算力不够，就不训练了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4a1135ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "overall accuracy=0.31481666500149863\n",
      "accuracy=0.7536217404336097\n",
      "precision=0.28827267475447715\n",
      "recall=0.998\n",
      "F1=0.447333034513671\n"
     ]
    }
   ],
   "source": [
    "#测试\n",
    "import tqdm\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model = torch.load('model2')\n",
    "    model.eval()\n",
    "    model=model.cuda()\n",
    "    hidden=rnn.initHidden()\n",
    "\n",
    "    target_label=7#自定义正例，其他为负例\n",
    " \n",
    "    #遍历整个数据集\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        x=x.cuda()\n",
    "        y=y.cuda()\n",
    "        hidden=hidden.cuda()\n",
    "        for j in range(x.shape[2]):\n",
    "            output, hidden = rnn(x[0,0,j], hidden)\n",
    "                \n",
    "        if i%1000==0:\n",
    "            print(i)\n",
    "        #求一个batch内各指标并累加\n",
    "        target_label=7\n",
    "        count(output.argmax(dim=1),y,target_label)\n",
    "    printParameter()\n",
    "    \n",
    "test()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pytorch2] *",
   "language": "python",
   "name": "conda-env-.conda-pytorch2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
