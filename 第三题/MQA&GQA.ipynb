{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b3d9c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "#创建多头注意力机制网络模型\n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        \n",
    "        ## 初始化Q、K、V投影矩阵\n",
    "        self.q_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        ## 输出线性层\n",
    "        self.o_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self,query,attention_mask=None):\n",
    "        batch_size = query.size()[0]\n",
    "        \n",
    "        #先传入线性层\n",
    "        query = self.q_linear(query)\n",
    "        key = self.k_linear(query)\n",
    "        value = self.v_linear(query)\n",
    "        \n",
    "        #分割张量，产生多个头\n",
    "        query = self.split_head(query)\n",
    "        key = self.split_head(key)\n",
    "        value = self.split_head(value)\n",
    "        \n",
    "        #使用缩放点积模型计算注意力分数\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2)) / torch.sqrt(torch.tensor(self.head_dim))\n",
    "\n",
    "        ## 对注意力分数进行归一化\n",
    "        attention_probs = torch.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        output = torch.matmul(attention_probs, value)\n",
    "        \n",
    "        ## 对注意力输出进行拼接\n",
    "        output = output.transpose(-1, -2).contiguous().view(batch_size, -1, self.head_dim * self.num_heads)\n",
    "        #线性输出\n",
    "        output = self.o_linear(output)\n",
    "        \n",
    "        return output,attention_probs\n",
    " \n",
    "    #分割张量元素，得到多个头\n",
    "    def split_head(self, x):\n",
    "        splited=torch.chunk(x, num_heads, dim=-1)\n",
    "        stacked=torch.stack((splited[:num_heads])).transpose(0,1)\n",
    "        return stacked\n",
    "    \"\"\"\n",
    "    (batch_size ,sequence_length ,hidden_size )->\n",
    "    (batch_size ,num_heads ,sequence_length , head_dim))\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59e31a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQueryAttention(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super(MultiQueryAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        \n",
    "        ## 初始化Q、K、V投影矩阵\n",
    "        self.q_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        ## 输出线性层\n",
    "        self.o_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self,query,attention_mask=None):\n",
    "        batch_size = query.size()[0]\n",
    "        \n",
    "        #先传入线性层\n",
    "        query = self.q_linear(query)\n",
    "        key = self.k_linear(query)\n",
    "        value = self.v_linear(query)\n",
    "        \n",
    "        #分割张量，key和value自有一个头\n",
    "        query = self.split_head(query)\n",
    "        key = self.split_head(key,head=1)\n",
    "        value = self.split_head(value,head=1)\n",
    "        \n",
    "        #使用缩放点积模型计算注意力分数\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2)) / torch.sqrt(torch.tensor(self.head_dim))\n",
    "        \n",
    "        ## 对注意力分数进行归一化\n",
    "        attention_probs = torch.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        output = torch.matmul(attention_probs, value)\n",
    "        \n",
    "        # 对注意力输出进行拼接\n",
    "        output = output.transpose(-1, -2).contiguous().view(batch_size, -1, self.head_dim * self.num_heads)\n",
    "        #线性输出\n",
    "        output = self.o_linear(output)\n",
    "        \n",
    "        return output,attention_probs\n",
    " \n",
    "    #分割张量元素，得到多个头\n",
    "    def split_head(self, x, head=None):\n",
    "        if head==None:\n",
    "            splited=torch.chunk(x, num_heads, dim=-1)\n",
    "            stacked=torch.stack((splited[:num_heads])).transpose(0,1)\n",
    "            return stacked\n",
    "            \"\"\"\n",
    "            (batch_size ,sequence_length ,hidden_size )->\n",
    "            (batch_size ,num_heads ,sequence_length , head_dim))\n",
    "            \"\"\"\n",
    "        else:\n",
    "            batch_size = x.size()[0]\n",
    "            return x.view(batch_size, -1, head, self.head_dim).transpose(1,2)\n",
    "            #(batch_size ,1 ,sequence_length , head_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a56f0d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupAttention(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads,group_num):\n",
    "        super(GroupAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        self.group_num=group_num\n",
    "        \n",
    "        ## 初始化Q、K、V投影矩阵\n",
    "        self.q_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        ## 输出线性层\n",
    "        self.o_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self,query,attention_mask=None):\n",
    "        batch_size = query.size()[0]\n",
    "        \n",
    "        #先传入线性层\n",
    "        query = self.q_linear(query)\n",
    "        key = self.k_linear(query)\n",
    "        value = self.v_linear(query)\n",
    "        \n",
    "        #分割张量，key和value自有一个头\n",
    "        query = self.split_head(query)\n",
    "        key = self.split_head(key,head=self.group_num)\n",
    "        value = self.split_head(value,head=self.group_num)\n",
    "        \n",
    "        #使用缩放点积模型计算注意力分数\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2) / self.head_dim**0.5)\n",
    "        \n",
    "        ## 对注意力分数进行归一化\n",
    "        attention_probs = torch.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        output = torch.matmul(attention_probs, value)\n",
    "        \n",
    "        # 对注意力输出进行拼接\n",
    "        output = output.transpose(-1, -2).contiguous().view(batch_size, -1, self.head_dim * self.num_heads)\n",
    "        #线性输出\n",
    "        output = self.o_linear(output)\n",
    "        \n",
    "        return output,attention_probs\n",
    " \n",
    "    #分割张量元素，得到多个头\n",
    "    def split_head(self, x, head=None):\n",
    "        batch_size = x.size()[0]\n",
    "        if head is None:\n",
    "            x=x.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1,2)#这样分割张量更方便\n",
    "            return x\n",
    "            \"\"\"\n",
    "            (batch_size ,sequence_length ,hidden_size )->\n",
    "            (batch_size ,num_heads ,sequence_length , head_dim))\n",
    "            \"\"\"\n",
    "        else:\n",
    "            x=x.view(batch_size, -1, head, self.head_dim).transpose(1,2).repeat(1,self.num_heads //self.group_num,1,1)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42fa771b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "seq_len = 10#序列长度\n",
    "hidden_size = 512#元素长度\n",
    "num_heads = 8\n",
    "group_num=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd1736e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 10, 512]), torch.Size([4, 8, 10, 10]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = torch.randn(batch_size, seq_len, hidden_size)\n",
    "\n",
    "# 创建多头注意力对象\n",
    "multi_head_attention = MultiHeadAttention(hidden_size, num_heads)\n",
    "# 计算多头自注意力，得到输出和注意力权重\n",
    "MHA_attention,MHA_attention_probs= multi_head_attention(query)\n",
    "\n",
    "MHA_attention.shape ,MHA_attention_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af3215a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4, 8, 8])\n",
      "torch.Size([4, 4, 8, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0274, -0.0297,  0.0498,  ...,  0.0218,  0.0803,  0.0251],\n",
       "          [-0.0412, -0.0445,  0.0269,  ..., -0.0015,  0.0863,  0.0086],\n",
       "          [-0.0314, -0.0184,  0.0244,  ..., -0.0115,  0.0972, -0.0123],\n",
       "          [-0.0853, -0.0346,  0.0305,  ..., -0.0049,  0.0899, -0.0071]],\n",
       " \n",
       "         [[ 0.0053, -0.0075, -0.1005,  ..., -0.0866, -0.0021,  0.0288],\n",
       "          [-0.0068,  0.0050, -0.0956,  ..., -0.0637,  0.0059, -0.0073],\n",
       "          [-0.0111, -0.0027, -0.0949,  ..., -0.0733,  0.0098, -0.0011],\n",
       "          [ 0.0029, -0.0149, -0.1058,  ..., -0.0759,  0.0048, -0.0007]],\n",
       " \n",
       "         [[-0.0466, -0.0379,  0.0785,  ..., -0.1324,  0.1220,  0.0476],\n",
       "          [-0.0412, -0.0339,  0.0413,  ..., -0.1403,  0.1458,  0.0595],\n",
       "          [-0.0190, -0.0591,  0.0495,  ..., -0.1382,  0.1322,  0.0639],\n",
       "          [-0.0296, -0.0357,  0.0555,  ..., -0.1452,  0.1161,  0.0492]],\n",
       " \n",
       "         [[ 0.0259,  0.0818, -0.0408,  ..., -0.0390, -0.1329,  0.0785],\n",
       "          [ 0.0331,  0.0915, -0.0596,  ..., -0.0375, -0.1452,  0.0565],\n",
       "          [ 0.0164,  0.0889, -0.0358,  ..., -0.0542, -0.1324,  0.0480],\n",
       "          [ 0.0546,  0.0950, -0.0358,  ..., -0.0433, -0.1314,  0.0689]]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " torch.Size([4, 4, 8, 8]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = torch.randn(batch_size,  hidden_size)\n",
    "\n",
    "# 创建多查询注意力对象\n",
    "multi_query_attention=MultiQueryAttention(hidden_size, num_heads)\n",
    "MQA_attention,MQA_attention_probs= multi_query_attention(query)\n",
    "\n",
    "print(MQA_attention_probs.shape)\n",
    "print(MQA_attention_probs.shape)\n",
    "MQA_attention ,MQA_attention_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d54e241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.1727, -0.0994,  0.0781,  ...,  0.0127,  0.0398, -0.1223]],\n",
       " \n",
       "         [[ 0.0168,  0.2205, -0.0086,  ..., -0.0311,  0.1290, -0.1132]],\n",
       " \n",
       "         [[-0.1978, -0.0029,  0.0129,  ...,  0.1279, -0.0065, -0.1365]],\n",
       " \n",
       "         [[ 0.1422, -0.0264, -0.2135,  ..., -0.2205,  0.1208, -0.3180]]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " torch.Size([4, 8, 1, 2]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = torch.randn(batch_size,  hidden_size)\n",
    "# 创建分组查询注意力对象\n",
    "group_attention=GroupAttention(hidden_size, num_heads,group_num)\n",
    "GQA_attention,GQA_attention_probs= group_attention(query)\n",
    "#输出和注意力权重\n",
    "GQA_attention,GQA_attention_probs.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pytorch2] *",
   "language": "python",
   "name": "conda-env-.conda-pytorch2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
